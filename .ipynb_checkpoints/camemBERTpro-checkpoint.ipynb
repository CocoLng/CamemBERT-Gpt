{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb6a5020-6161-44ce-a58f-115a63c9a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def DL_oscar_subset(n_subset:int=1e4)->pd.DataFrame:\n",
    "    dataset_stream = load_dataset(\"oscar\", \"unshuffled_deduplicated_fr\", split=\"train\", streaming=True)\n",
    "    \n",
    "    subset = []\n",
    "    \n",
    "    for i, example in tqdm(enumerate(dataset_stream), total=n_subset):\n",
    "        subset.append(example)\n",
    "        if i+1 >= n_subset:\n",
    "            break\n",
    "    return pd.DataFrame(subset)\n",
    "\n",
    "def save_dataset(dataset, name=\"oscar_subset.csv\"):\n",
    "    dataset.to_csv(name, index=False)\n",
    "\n",
    "\n",
    "def get_oscar_dataset(n_subset=1e4, name=\"oscar_subset.csv\"):\n",
    "    file_path = Path.cwd() / name\n",
    "    \n",
    "    # Si le fichier n'existe pas, on le télécharge\n",
    "    if not file_path.exists():\n",
    "        print(f\"Le fichier {name} n'existe pas. Téléchargement en cours...\")\n",
    "        subset = DL_oscar_subset(n_subset)\n",
    "        save_dataset(subset)\n",
    "        print(\"Téléchargement et enregistrement du dataset effectués.\")\n",
    "    \n",
    "    # Chargement du dataset\n",
    "    dataset = pd.read_csv(name)\n",
    "    \n",
    "    if len(dataset) < n_subset:\n",
    "        print(f\"Le dataset contient moins de {n_subset} exemples. Téléchargement supplémentaire...\")\n",
    "        subset = DL_oscar_subset(n_subset)\n",
    "        save_dataset(subset)\n",
    "        dataset = pd.read_csv(name)\n",
    "    \n",
    "    # Retourne un sous-ensemble du dataset (limité à n_subset exemples)\n",
    "    return dataset.head(int(n_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "913bae9a-8019-40ad-ba20-9cb01fdd82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subset = 1.2e4\n",
    "dataset = get_oscar_dataset(n_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05956c-6dbd-45e2-9c06-51ec3b1a3432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb341806-1384-4708-b5a3-529ada2e2796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6931ad58-3944-4dd5-81ba-71f1dcb66cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import CamembertTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4632406d-2375-4804-9486-8c97ba68e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171f05ca-0b66-4f4f-8e2f-9f57c0846f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_mask_data(texts, tokenizer, max_length=128, mlm_probability=0.15):\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # Create a mask\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # Replace masked input tokens with tokenizer.mask_token_id\n",
    "    input_ids[masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    return input_ids, attention_mask, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b401929a-5842-4ec5-8c4e-f91c353b2ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OscarDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128, mlm_probability=0.15):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "        self.mlm_probability = mlm_probability\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        input_ids, attention_mask, labels = preprocess_and_mask_data(\n",
    "            [text], self.tokenizer, self.max_length, self.mlm_probability\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': input_ids.squeeze(0),\n",
    "            'attention_mask': attention_mask.squeeze(0),\n",
    "            'labels': labels.squeeze(0)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1099701e-b358-440a-9c07-82f4043ec1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dataset['text']\n",
    "\n",
    "train_dataset = OscarDataset(texts, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049d376-c163-425b-a5a7-56a9c4446cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14e8f176-b237-4592-933b-9673fc371da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import CamembertConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5160ab9-8ece-4972-aedf-92d225044030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamembertForMaskedLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Load the configuration\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.can_generate = lambda : False # a verifier ; sans imposible de faire l'inference fin de notebook\n",
    "\n",
    "        # Embeddings\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(1, config.hidden_size)  # CamemBERT uses only one segment\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.hidden_size,\n",
    "            nhead=config.num_attention_heads,\n",
    "            dim_feedforward=config.intermediate_size,\n",
    "            dropout=config.hidden_dropout_prob,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_hidden_layers)\n",
    "\n",
    "        # MLM Head\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize weights following the BERT initialization\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                nn.init.constant_(module.weight, 1.0)\n",
    "        if isinstance(self.lm_head, nn.Linear) and self.lm_head.bias is not None:\n",
    "            nn.init.constant_(self.lm_head.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get input embeddings\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        token_type_ids = torch.zeros_like(input_ids, dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        inputs_embeds = self.embeddings(input_ids) + self.position_embeddings(position_ids) + self.token_type_embeddings(token_type_ids)\n",
    "        hidden_states = self.layer_norm(inputs_embeds)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Apply attention mask\n",
    "        if attention_mask is not None:\n",
    "            # Invert the attention mask for nn.TransformerEncoder\n",
    "            attention_mask = attention_mask.bool()\n",
    "            attention_mask = ~attention_mask\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "        # Transformer encoder\n",
    "        hidden_states = self.encoder(hidden_states.transpose(0, 1), src_key_padding_mask=attention_mask)\n",
    "        hidden_states = hidden_states.transpose(0, 1)\n",
    "\n",
    "        # MLM Head\n",
    "        prediction_scores = self.lm_head(hidden_states)\n",
    "\n",
    "        output = {'logits': prediction_scores}\n",
    "\n",
    "        # Compute loss if labels are provided\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # Shift prediction scores and labels for computing loss\n",
    "            loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            output['loss'] = loss\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3694549b-5524-436e-bd58-ad20326ba58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dorianmariani/miniconda3/envs/camemBERTenv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = CamembertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_position_embeddings=tokenizer.model_max_length,\n",
    "    num_hidden_layers=6,  # Use fewer layers for faster training\n",
    "    num_attention_heads=8,\n",
    "    hidden_size=512,\n",
    "    intermediate_size=2048,\n",
    "    hidden_act='gelu',\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "model = CamembertForMaskedLM(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f113d72-41de-48af-8caf-1cebff38ff6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamembertForMaskedLM(\n",
       "  (embeddings): Embedding(32005, 512, padding_idx=1)\n",
       "  (position_embeddings): Embedding(512, 512)\n",
       "  (token_type_embeddings): Embedding(1, 512)\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32005, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f6c19ce-b3b0-4371-a1e0-7924a7e8a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dorianmariani/miniconda3/envs/camemBERTenv/lib/python3.13/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f956d07-67ff-4879-a554-da0cc0097056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1500/1500 [17:36<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 8.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1500/1500 [21:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 7.6863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1500/1500 [18:51<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 7.5965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85ff3990-3d70-44d5-8091-b181ca1535d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/camembertPRO_custom_tokenizer/tokenizer_config.json',\n",
       " './models/camembertPRO_custom_tokenizer/special_tokens_map.json',\n",
       " './models/camembertPRO_custom_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = './models/camembertPRO_custom_model.pth'\n",
    "tokenizer_save_path = './models/camembertPRO_custom_tokenizer'\n",
    "\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01593f27-1535-4227-a942-88eec393353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Paris est la capitale de la de ., Score: 0.0476\n",
      "Prediction: Paris est la capitale de la, ., Score: 0.0289\n",
      "Prediction: Paris est la capitale de la. ., Score: 0.0244\n",
      "Prediction: Paris est la capitale de la la ., Score: 0.0203\n",
      "Prediction: Paris est la capitale de las ., Score: 0.0203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9402/487970788.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_load.load_state_dict(torch.load(model_save_path))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import pipeline\n",
    "from transformers import CamembertTokenizerFast\n",
    "\n",
    "model_load = CamembertForMaskedLM(config)\n",
    "tokenizer_load = CamembertTokenizerFast.from_pretrained(tokenizer_save_path)\n",
    "model_load.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model_load,\n",
    "    tokenizer=tokenizer_load,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test sentence\n",
    "sentence = \"Paris est la capitale de la <mask>.\"\n",
    "results = fill_mask(sentence)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Prediction: {result['sequence']}, Score: {result['score']:.4f}\") \n",
    "\n",
    "# ouch, mais techniquement ca marche \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad844a-bbfb-48c3-bc1f-96dd64f2b7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39245f1f-bdc2-4670-8b40-64358eac5f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c65a3-36de-4c8c-9d24-f1104eab6a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb2bf1-eef4-42a5-8395-17633727d9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (camemBERTenv)",
   "language": "python",
   "name": "camembertenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
